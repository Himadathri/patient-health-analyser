{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Copyright (c) Microsoft Corporation.\n",
        "\n",
        "Licensed under the MIT License.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DISCLAIMER\n",
        "By accessing this code, you acknowledge that the code is not designed, intended, or made available: (1) as a medical device(s); (2) for the diagnosis of disease or other conditions, or in the cure, mitigation, treatment or prevention of a disease or other conditions; or (3) as a substitute for professional medical advice, diagnosis, treatment, or judgment. Do not use this code to replace, substitute, or provide professional medical advice, diagnosis, treatment, or judgement. You are solely responsible for ensuring the regulatory, legal, and/or contractual compliance of any use of the code, including obtaining any authorizations or consents, and any solution you choose to build that incorporates this code in whole or in part.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Recommendations \n",
        "Load the test data and apply to the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Library Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "\n",
        "data_lake_account_name = \"\"\n",
        "file_system_name = \"raw\"\n",
        "\n",
        "subscription_id = \"\" \n",
        "resource_group = \"\" \n",
        "workspace_name = \"\" \n",
        "workspace_region = \"\"\n",
        "\n",
        "experiment_name = \"DiabetesPredictionExperiment\"\n",
        "autoMLRunId = \"\"\n",
        "aks_target_name = ''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# set transformed data schema\n",
        "transformedSchema = StructType([StructField(\"race\", StringType(), True), \n",
        "                    StructField(\"gender\", StringType(), True), \n",
        "                    StructField(\"age\", StringType(), True) , \n",
        "                    StructField(\"admission_type_id\", StringType(), True), \n",
        "                    StructField(\"discharge_disposition_id\", StringType(), True), \n",
        "                    StructField(\"admission_source_id\", StringType(), True), \n",
        "                    StructField(\"time_in_hospital\", StringType(), True), \n",
        "                    StructField(\"payer_code\", StringType(), True), \n",
        "                    StructField(\"num_lab_procedures\", StringType(), True), \n",
        "                    StructField(\"num_procedures\", StringType(), True), \n",
        "                    StructField(\"num_medications\", StringType(), True),\n",
        "                    StructField(\"number_outpatient\", StringType(), True), \n",
        "                    StructField(\"number_emergency\", StringType(), True), \n",
        "                    StructField(\"number_inpatient\", StringType(), True), \n",
        "                    StructField(\"number_diagnoses\", StringType(), True), \n",
        "                    StructField(\"max_glu_serum\", StringType(), True), \n",
        "                    StructField(\"A1Cresult\", StringType(), True), \n",
        "                    StructField(\"metformin\", StringType(), True), \n",
        "                    StructField(\"repaglinide\", StringType(), True), \n",
        "                    StructField(\"nateglinide\", StringType(), True), \n",
        "                    StructField(\"chlorpropamide\", StringType(), True), \n",
        "                    StructField(\"glimepiride\", StringType(), True),\n",
        "                    StructField(\"glipizide\", StringType(), True), \n",
        "                    StructField(\"glyburide\", StringType(), True), \n",
        "                    StructField(\"tolbutamide\", StringType(), True), \n",
        "                    StructField(\"pioglitazone\", StringType(), True), \n",
        "                    StructField(\"rosiglitazone\", StringType(), True), \n",
        "                    StructField(\"acarbose\", StringType(), True), \n",
        "                    StructField(\"miglitol\", StringType(), True), \n",
        "                    StructField(\"tolazamide\", StringType(), True),\n",
        "                    StructField(\"insulin\", StringType(), True), \n",
        "                    StructField(\"glyburide-metformin\", StringType(), True), \n",
        "                    StructField(\"metformin-rosiglitazone\", StringType(), True), \n",
        "                    StructField(\"change\", StringType(), True), \n",
        "                    StructField(\"diabetesMed\", StringType(), True), \n",
        "                    StructField(\"FirstName\", StringType(), True), \n",
        "                    StructField(\"LastName\", StringType(), True),\n",
        "                    StructField(\"Id\", StringType(), True), \n",
        "                    StructField(\"spec_InternalMedicine\", BooleanType(), True), \n",
        "                    StructField(\"spec_Emergency/Trauma\", BooleanType(), True),\n",
        "                    StructField(\"spec_Family/GeneralPractice\", BooleanType(), True), \n",
        "                    StructField(\"spec_Cardiology\", BooleanType(), True), \n",
        "                    StructField(\"spec_Surgery-General\", BooleanType(), True), \n",
        "                    StructField(\"diag_428\", BooleanType(), True), \n",
        "                    StructField(\"diag_250\", BooleanType(), True), \n",
        "                    StructField(\"diag_276\", BooleanType(), True), \n",
        "                    StructField(\"diag_414\", BooleanType(), True), \n",
        "                    StructField(\"diag_401\", BooleanType(), True),  \n",
        "                    StructField(\"diag_427\", BooleanType(), True), \n",
        "                    StructField(\"diag_599\", BooleanType(), True), \n",
        "                    StructField(\"diag_496\", BooleanType(), True), \n",
        "                    StructField(\"diag_403\", BooleanType(), True), \n",
        "                    StructField(\"diag_486\", BooleanType(), True),  \n",
        "                    StructField(\"is_readmitted\", BooleanType(), True)\n",
        "                    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load Data from Azure Data Lake \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split \n",
        "import pandas as pd\n",
        "\n",
        "df_train = spark.read.format(\"csv\").load(f\"abfss://{file_system_name}@{data_lake_account_name}.dfs.core.windows.net/DatasetDiabetes/preparedtraindata/\",header=True,schema=transformedSchema)\n",
        "df_train = df_train.toPandas()\n",
        "outcome_column = 'is_readmitted'\n",
        "\n",
        "id_column = 'Id'\n",
        "df_train = df_train.drop(id_column,axis=1)\n",
        "\n",
        "\n",
        "#%% Split data for validation\n",
        "X = df_train.drop(outcome_column, axis=1) \n",
        "y = df_train[outcome_column] \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Connect to Azure Machine Learning Workspace, Experiment and Load Best Run \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#save the model to a local file\n",
        "import azureml.core\n",
        "\n",
        "\n",
        "from azureml.core import Workspace\n",
        "ws = Workspace(workspace_name = workspace_name,\n",
        "               subscription_id = subscription_id,\n",
        "               resource_group = resource_group)\n",
        "ws.write_config()   \n",
        "\n",
        "from azureml.core.experiment import Experiment\n",
        "from azureml.core.workspace import Workspace\n",
        "from azureml.train.automl.run import AutoMLRun\n",
        "from azureml.train.automl import AutoMLConfig\n",
        "\n",
        "experiment = Experiment(workspace = ws, name = experiment_name)\n",
        "previous_automl_run = AutoMLRun(experiment, autoMLRunId, outputs = None)\n",
        "automl_run = previous_automl_run\n",
        "\n",
        "best_run, fitted_model = automl_run.get_output()\n",
        "\n",
        "\n",
        "from sklearn.externals import joblib\n",
        "model_path = 'diabetesmodel'\n",
        "joblib.dump(fitted_model, model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.interpret.scoring.scoring_explainer import TreeScoringExplainer, save\n",
        "\n",
        "from interpret.ext.glassbox import LGBMExplainableModel\n",
        "from azureml.interpret.mimic_wrapper import MimicWrapper\n",
        "\n",
        "from azureml.train.automl.runtime.automl_explain_utilities import AutoMLExplainerSetupClass, automl_setup_model_explanations\n",
        "automl_explainer_setup_obj = automl_setup_model_explanations(fitted_model, X=X_train,\n",
        "                                                             X_test=X_test, y=y_train,\n",
        "                                                             task='classification')   \n",
        "\n",
        "explainer = MimicWrapper(ws, automl_explainer_setup_obj.automl_estimator, LGBMExplainableModel,\n",
        "                         init_dataset=automl_explainer_setup_obj.X_transform, run=best_run,\n",
        "                         features=automl_explainer_setup_obj.engineered_feature_names,\n",
        "                         feature_maps=[automl_explainer_setup_obj.feature_map],\n",
        "                         classes=automl_explainer_setup_obj.classes)                                                               \n",
        "\n",
        "#Initialize the ScoringExplainer\n",
        "scoring_explainer = TreeScoringExplainer(explainer.explainer, feature_maps=[automl_explainer_setup_obj.feature_map])\n",
        "\n",
        "#Pickle scoring explainer locally\n",
        "save(scoring_explainer, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Register the Model \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.core.model import Model\n",
        "\n",
        "model_name = \"diabetesmodel\"\n",
        "registered_model = Model.register(model_path = model_path, # this points to a local file\n",
        "                       model_name = model_name, # name the model is registered as\n",
        "                       tags = {'type': \"classification\"}, \n",
        "                       description = \"Diabetes Classifier\", \n",
        "                       workspace = ws)\n",
        "\n",
        "\n",
        "exp_model_name = \"scoring_explainer.pkl\"\n",
        "exp_model_path = \"scoring_explainer.pkl\"\n",
        "exp_registered_model = Model.register(model_path = exp_model_path, # this points to a local file\n",
        "                       model_name = exp_model_name, # name the model is registered as\n",
        "                       tags = {'type': \"scoring explainer\"}, \n",
        "                       description = \"Diabetes Readmission Classifier Explainer\", \n",
        "                       workspace = ws)                         "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_additional_features(df): \n",
        "    to_drop = ['acetohexamide', 'troglitazone', 'examide', 'citoglipton',\n",
        "           'glipizide-metformin', 'glimepiride-pioglitazone',\n",
        "           'metformin-pioglitazone', 'weight', 'patient_nbr', 'encounter_id']\n",
        "    df.drop(to_drop, axis=1, inplace=True, errors = 'ignore')\n",
        "    df_transformed = df.replace('?', np.nan) \n",
        "    \n",
        "    spec_counts_raw = {\"specs\": ['InternalMedicine', 'Emergency/Trauma', 'Family/GeneralPractice','Cardiology',\n",
        "                       'Surgery-General'], \"num patients\": [14635,  7565,  7440,  5352,  3099]}\n",
        "\n",
        "    df_transformed['medical_specialty'] = df_transformed['medical_specialty'].replace(np.nan, \"NaNSpec\")\n",
        "    spec_counts = pd.DataFrame(data = spec_counts_raw)\n",
        "    spec_thresh = 5\n",
        "    for (index, row) in spec_counts.head(spec_thresh).iterrows():\n",
        "        spec = row['specs']\n",
        "        new_col = 'spec_' + str(spec)\n",
        "        df_transformed[new_col] = (df_transformed.medical_specialty == spec)\n",
        "\n",
        "    diag_counts_raw = {\"icd9value\": ['428', '250', '276', '414', '401', '427', '599', '496', '403', '486'],\n",
        "                    'num patients w diag': [18101., 17861., 13816., 12895., 12371., 11757.,  6824.,  5990.,5693., 5455.]}\n",
        "\n",
        "    diag_counts = pd.DataFrame(diag_counts_raw, columns = [ 'icd9value', 'num patients w diag'])\n",
        "\n",
        "    diag_thresh = 10\n",
        "    for (index, row) in diag_counts.head(diag_thresh).iterrows():\n",
        "        icd9 = row['icd9value']\n",
        "        new_col = 'diag_' + str(icd9)\n",
        "        df_transformed[new_col] = (df_transformed.diag_1 == icd9)|(df_transformed.diag_2 == icd9)|(df_transformed.diag_3 == icd9)\n",
        "\n",
        "    df_transformed = df_transformed.reset_index(drop=True)\n",
        "\n",
        "    df_transformed2 = pd.DataFrame(df_transformed, copy=True) #preserve df_transformed so I can rerun this step\n",
        "    df_transformed2['age'] = df_transformed2.age.str.extract('(\\d+)-\\d+')\n",
        "\n",
        "    to_drop = ['acetohexamide', 'troglitazone', 'examide', 'citoglipton',\n",
        "        'glipizide-metformin', 'glimepiride-pioglitazone',\n",
        "        'metformin-pioglitazone', 'weight', 'medical_specialty', 'diag_2',\n",
        "        'diag_1', 'diag_3', 'patient_nbr', 'encounter_id']\n",
        "    df_transformed2.drop(to_drop, axis=1, inplace=True,errors = 'ignore')\n",
        "\n",
        "    df_transformed2 = df_transformed2.reset_index(drop=True)\n",
        "\n",
        "    df_transformed2['readmitted'].value_counts()\n",
        "\n",
        "    df = pd.DataFrame(df_transformed2)\n",
        "\n",
        "    #Imputing with outlying value since we are focusing on tree based methods\n",
        "    df = df.fillna(-9999) \n",
        "\n",
        "    df = df.reset_index(drop=True)\n",
        "    df.dtypes\n",
        "    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_test = spark.read.format(\"csv\").load(f\"abfss://{file_system_name}@{data_lake_account_name}.dfs.core.windows.net/DatasetDiabetes/preparedtestdata/\",header=True,multiLine=True)\n",
        "df_test = df_test.toPandas()\n",
        "outcome_column = 'readmitted'\n",
        "df_test = df_test.drop(outcome_column,axis=1)\n",
        "\n",
        "df_test = df_test.head(2)\n",
        "id_column = 'Id'\n",
        "df_test = df_test.drop(id_column,axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scoring_script = \"\"\"\n",
        "import json\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import azureml.train.automl\n",
        "from sklearn.externals import joblib\n",
        "from azureml.core.model import Model\n",
        "from azureml.train.automl.runtime.automl_explain_utilities import automl_setup_model_explanations\n",
        "\n",
        "def create_additional_features(df): \n",
        "    to_drop = ['acetohexamide', 'troglitazone', 'examide', 'citoglipton',\n",
        "           'glipizide-metformin', 'glimepiride-pioglitazone',\n",
        "           'metformin-pioglitazone', 'weight', 'patient_nbr', 'encounter_id']\n",
        "    df.drop(to_drop, axis=1, inplace=True, errors = 'ignore')\n",
        "    df_transformed = df.replace('?', np.nan) \n",
        "    \n",
        "    spec_counts_raw = {\"specs\": ['InternalMedicine', 'Emergency/Trauma', 'Family/GeneralPractice','Cardiology',\n",
        "                       'Surgery-General'], \"num patients\": [14635,  7565,  7440,  5352,  3099]}\n",
        "\n",
        "    df_transformed['medical_specialty'] = df_transformed['medical_specialty'].replace(np.nan, \"NaNSpec\")\n",
        "    spec_counts = pd.DataFrame(data = spec_counts_raw)\n",
        "    spec_thresh = 5\n",
        "    for (index, row) in spec_counts.head(spec_thresh).iterrows():\n",
        "        spec = row['specs']\n",
        "        new_col = 'spec_' + str(spec)\n",
        "        df_transformed[new_col] = (df_transformed.medical_specialty == spec)\n",
        "\n",
        "    diag_counts_raw = {\"icd9value\": ['428', '250', '276', '414', '401', '427', '599', '496', '403', '486'],\n",
        "                    'num patients w diag': [18101., 17861., 13816., 12895., 12371., 11757.,  6824.,  5990.,5693., 5455.]}\n",
        "\n",
        "    diag_counts = pd.DataFrame(diag_counts_raw, columns = [ 'icd9value', 'num patients w diag'])\n",
        "\n",
        "    diag_thresh = 10\n",
        "    for (index, row) in diag_counts.head(diag_thresh).iterrows():\n",
        "        icd9 = row['icd9value']\n",
        "        new_col = 'diag_' + str(icd9)\n",
        "        df_transformed[new_col] = (df_transformed.diag_1 == icd9)|(df_transformed.diag_2 == icd9)|(df_transformed.diag_3 == icd9)\n",
        "\n",
        "    df_transformed = df_transformed.reset_index(drop=True)\n",
        "\n",
        "    df_transformed2 = pd.DataFrame(df_transformed, copy=True) #preserve df_transformed so I can rerun this step\n",
        "    df_transformed2['age'] = df_transformed2.age.str.extract('(\\d+)-\\d+')\n",
        "\n",
        "    to_drop = ['acetohexamide', 'troglitazone', 'examide', 'citoglipton',\n",
        "        'glipizide-metformin', 'glimepiride-pioglitazone',\n",
        "        'metformin-pioglitazone', 'weight', 'medical_specialty', 'diag_2',\n",
        "        'diag_1', 'diag_3', 'patient_nbr', 'encounter_id']\n",
        "    df_transformed2.drop(to_drop, axis=1, inplace=True,errors = 'ignore')\n",
        "\n",
        "    df_transformed2 = df_transformed2.reset_index(drop=True)\n",
        "\n",
        "    # df_transformed2['readmitted'].value_counts()\n",
        "\n",
        "    df = pd.DataFrame(df_transformed2)\n",
        "\n",
        "    #Imputing with outlying value since we are focusing on tree based methods\n",
        "    df = df.fillna(-9999) \n",
        "\n",
        "    df = df.reset_index(drop=True)\n",
        "    df.dtypes\n",
        "    \n",
        "    return df\n",
        "\n",
        "def init():\n",
        "    global model\n",
        "    global scoring_explainer\n",
        "    # This name is model.id of model that we want to deploy deserialize the model file back\n",
        "    model_path = Model.get_model_path(model_name = 'diabetesmodel')\n",
        "    model = joblib.load(model_path)\n",
        "    \n",
        "    scoring_explainer_path = Model.get_model_path(model_name = 'scoring_explainer.pkl')\n",
        "    scoring_explainer = joblib.load(scoring_explainer_path)\n",
        "\n",
        "def run(input_json):     \n",
        "    try:\n",
        "        data_df = pd.read_json(input_json, orient='records').head(1)\n",
        "        data_df = create_additional_features(data_df)\n",
        "        stacked_data = pd.DataFrame(data_df.stack().reset_index())\n",
        "        stacked_data.columns = ['Ind','Column','Value']\n",
        "        stacked_data = stacked_data[['Column','Value']]\n",
        "            \n",
        "        # Get the predictions...\n",
        "        # prediction = model.predict(data_df)\n",
        "        prediction = pd.DataFrame(model.predict_proba(data_df),columns=model.y_transformer.inverse_transform(model.classes_)).T.iloc[0,0]\n",
        "        prediction = np.round(prediction * 100,2)\n",
        "\n",
        "        automl_explainer_setup_obj = automl_setup_model_explanations(model,X_test=data_df, task='classification')\n",
        "        raw_local_importance_values = scoring_explainer.explain(automl_explainer_setup_obj.X_test_transform, get_raw=True)\n",
        "\n",
        "        stacked_data['raw_imp'] = raw_local_importance_values[0]\n",
        "        stacked_data = stacked_data.sort_values('raw_imp',ascending = False).head(10)\n",
        "        #stacked_data['raw_imp'] = stacked_data['raw_imp'] * 100\n",
        "        stacked_data = stacked_data.round(2)\n",
        "    except Exception as e:\n",
        "        prediction = np.array([str(e)])\n",
        "        stacked_data = pd.DataFrame([str(e)])\n",
        " \n",
        "    return {'predictions': prediction.tolist(),\n",
        "            'raw_local_importance_values': stacked_data.values.tolist()}\n",
        "\"\"\"\n",
        "exec(scoring_script)\n",
        "with open(\"scoring_script.py\", \"w\") as file:\n",
        "    file.write(scoring_script)\n",
        "    \n",
        "scoring_script_file_name = 'scoring_script.py'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "json_test_data = df_test.head(1).to_json(orient='records')\n",
        "print(json_test_data)\n",
        "init()\n",
        "run(json_test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# obtain conda dependencies from the automl run and save the file locally\n",
        "from azureml.core import Environment\n",
        "from azureml.core.environment import CondaDependencies\n",
        "conda_dep = CondaDependencies()\n",
        "environment_config_file = 'diabetes_conda_env.yml'\n",
        "best_run.download_file('outputs/conda_env_v_1_0_0.yml', environment_config_file)\n",
        "with open('diabetes_conda_env.yml', 'r') as f:\n",
        "    print(f.read())\n",
        "\n",
        "# create the environment based on the saved conda dependencies file\n",
        "myenv = Environment.from_conda_specification(name=\"diabetesenv\", file_path=environment_config_file)\n",
        "conda_dep.add_pip_package(\"shap==0.35.0\")\n",
        "conda_dep.add_pip_package(\"azureml-train-automl-runtime==1.32.0\")\n",
        "conda_dep.add_pip_package(\"inference-schema\")\n",
        "conda_dep.add_pip_package(\"azureml-interpret==1.32.0\")\n",
        "conda_dep.add_pip_package(\"azureml-defaults==1.32.0\")\n",
        "conda_dep.add_conda_package(\"numpy>=1.16.0,<1.19.0\")\n",
        "conda_dep.add_conda_package(\"pandas==0.25.1\")\n",
        "conda_dep.add_conda_package(\"scikit-learn==0.22.1\")\n",
        "conda_dep.add_conda_package(\"py-xgboost<=0.90\")\n",
        "conda_dep.add_conda_package(\"fbprophet==0.5\")\n",
        "conda_dep.add_conda_package(\"holidays==0.9.11\")\n",
        "conda_dep.add_conda_package(\"psutil>=5.2.2,<6.0.0\")\n",
        "myenv.python.conda_dependencies=conda_dep\n",
        "myenv.register(workspace=ws)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deploy Model to AKS Cluster \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.core.compute import AksCompute, ComputeTarget\n",
        "from azureml.core.model import InferenceConfig\n",
        "from azureml.core.webservice import AksWebservice\n",
        "from azureml.core.webservice import Webservice\n",
        "\n",
        "# Configure and deploy the web service to Azure Container Instances\n",
        "inference_config = InferenceConfig(environment=myenv, entry_script=scoring_script_file_name)\n",
        "aks_config = AksWebservice.deploy_configuration(cpu_cores = 1, memory_gb= 2, tags = { 'type' : 'automl-classification'}, description='AutoML Diabetes Readmission Classifier Service')\n",
        "aks_service_name = 'diabetes-readmission-service-aks'\n",
        "aks_target = AksCompute(ws,aks_target_name)\n",
        "aks_service = Model.deploy(ws, aks_service_name, [exp_registered_model,registered_model], inference_config, aks_config, aks_target)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "aks_service.wait_for_deployment(show_output = True)\n",
        "print(aks_service.state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "json_test_data = df_test.head(1).to_json()\n",
        "aks_service.run(json_test_data)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": false,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
